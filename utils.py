"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from dotenv import load_dotenv
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct


############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def get_llm_response(chat_message):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
    if st.session_state.mode == ct.ANSWER_MODE_1:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…æ–‡æ›¸æ¤œç´¢ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_DOC_SEARCH
    else:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…å•ã„åˆã‚ã›ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    # èª²é¡Œ4 LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    from langchain.prompts import PromptTemplate
    #context ã‚’æ˜ç¤ºçš„ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¸¡ã™
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )


    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )

    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
    question_answer_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=question_answer_prompt,
        document_variable_name="context",
        )
    # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# ã‚¨ãƒ©ãƒ¼å¯¾ç­–
    # # LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
    # llm_response = chain.invoke(
    #     {"input": chat_message, 
    #     "chat_history": st.session_state.chat_history,
    #     })
    
    try:
        llm_response = chain.invoke({
            "input": chat_message,
            "chat_history": st.session_state.chat_history
        })
    except Exception as e:
        print("âŒ LLMå‘¼ã³å‡ºã—æ™‚ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ:", e)
        st.error("å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ã¯ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        return {"answer": ct.NO_DOC_MATCH_ANSWER, "context": []}


    # ğŸ” context ã®ä¸­èº«ã‚’è¡¨ç¤ºï¼ˆé–‹ç™ºç”¨ï¼‰
    if "context" in llm_response:
        st.write("ğŸ” contextã®ä¸­èº«:")
        for i, doc in enumerate(llm_response["context"]):
            st.write(f"{i}: {doc.metadata}")

    
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=chat_message), llm_response["answer"]])

    return llm_response

# èª²é¡Œ4ã®ãŸã‚ã®è¿½åŠ 
from langchain_community.document_loaders import PyMuPDFLoader

class PageAwarePDFLoader(PyMuPDFLoader):
    def load(self):
        docs = super().load()
        for i, doc in enumerate(docs):
            if "page" not in doc.metadata:
                doc.metadata["page"] = i  # ä½ç½®ã«å¿œã˜ã¦ä»®ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’è£œå®Œ
        return docs
